{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5600, 64, 64, 3), Labels shape: (5600,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import load_model\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, load the model with compile=False to avoid optimizer warnings\n",
    "model = load_model(r'isFinal.h5', compile=False)\n",
    "\n",
    "# Recompile the model with the necessary metrics and optimizer\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "def load_images_from_folder(folder, label, max_images=3500):\n",
    "    images = []\n",
    "    labels = []\n",
    "    all_files = os.listdir(folder)\n",
    "\n",
    "    # Shuffle and select a subset of files\n",
    "    np.random.shuffle(all_files)\n",
    "    max_images = max_images or len(all_files) // 2\n",
    "\n",
    "    for filename in all_files[:max_images]:\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (64, 64))  # Resize to 128x128\n",
    "            images.append(resized_img)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays and add channel dimension\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load the images\n",
    "real_images, real_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Train\\Real', 0)\n",
    "fake_images, fake_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Train\\Fake', 1)\n",
    "\n",
    "# Combine real and fake data\n",
    "x = np.concatenate((real_images, fake_images), axis=0)\n",
    "y = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "    \n",
    "# Split the data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the pixel values\n",
    "xtrain = xtrain / 255.0\n",
    "xtest = xtest / 255.0\n",
    "\n",
    "print(f\"Training data shape: {xtrain.shape}, Labels shape: {ytrain.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 395ms/step - accuracy: 0.9456 - loss: 0.1262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13000768423080444, 0.9494285583496094]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_images, test_real_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Test\\Real', 0)\n",
    "test_fake_images, test_fake_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Test\\Fake', 1)\n",
    "\n",
    "# Combine real and fake data\n",
    "testData = np.concatenate((test_real_images, test_fake_images), axis=0)\n",
    "testLabels = np.concatenate((test_real_labels, test_fake_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 53ms/step - accuracy: 0.9277 - loss: 27.1221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[51.5689582824707, 0.8830000162124634]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testData,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_real_images, val_real_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Validation\\Real', 0,max_images=None)\n",
    "val_fake_images, val_fake_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Validation\\Fake', 1,max_images=None)\n",
    "\n",
    "# Combine real and fake data\n",
    "valData = np.concatenate((val_real_images, val_fake_images), axis=0)\n",
    "valLabels = np.concatenate((val_real_labels, val_fake_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 413ms/step - accuracy: 0.9252 - loss: 0.1793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22787314653396606, 0.9121899008750916]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valData,valLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step\n",
      "[[0.54456055]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = r'D:\\SIH project\\20210119_210738.jpg'\n",
    "img = image.load_img(img_path, target_size=(128, 128))  # Resize to 224x224 if that's what your model expects\n",
    "img_array = image.img_to_array(img)  # Convert to array\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "img_array = img_array / 255.0  # Normalize the image\n",
    "\n",
    "# Load your pre-trained deep fake analysis model\n",
    "#model = tf.keras.models.load_model('path_to_your_model.h5')\n",
    "\n",
    "# Pass the image through the model\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Interpret the results\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
