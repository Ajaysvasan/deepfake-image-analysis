{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19fc0adf-610f-4ce8-bbef-4ec566f91c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5600, 128, 128, 1), Labels shape: (5600,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder, label, max_images=3500):\n",
    "    images = []\n",
    "    labels = []\n",
    "    all_files = os.listdir(folder)\n",
    "\n",
    "    # Shuffle and select a subset of files\n",
    "    np.random.shuffle(all_files)\n",
    "    max_images = max_images or len(all_files) // 2\n",
    "\n",
    "    for filename in all_files[:max_images]:\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "        if img is not None:\n",
    "            resized_img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
    "            images.append(resized_img)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays and add channel dimension\n",
    "    images = np.array(images).reshape(-1, 128, 128, 1)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load the images\n",
    "real_images, real_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Train\\Real', 0)\n",
    "fake_images, fake_labels = load_images_from_folder(r'D:\\SIH project\\dataset2\\Dataset\\Train\\Fake', 1)\n",
    "\n",
    "# Combine real and fake data\n",
    "x = np.concatenate((real_images, fake_images), axis=0)\n",
    "y = np.concatenate((real_labels, fake_labels), axis=0)\n",
    "\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Normalize the pixel values\n",
    "xtrain = xtrain / 255.0\n",
    "xtest = xtest/255.0\n",
    "\n",
    "print(f\"Training data shape: {xtrain.shape}, Labels shape: {ytrain.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61cbd53f-983f-43dc-9dc6-84bd0209a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "##using CNN model - Test One\n",
    "\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),  # Grayscale input\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd48bac4-fe2b-4f00-8b29-b37929b4ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 250ms/step - accuracy: 0.5571 - loss: 0.6841 - val_accuracy: 0.6986 - val_loss: 0.5905\n",
      "Epoch 2/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 253ms/step - accuracy: 0.7241 - loss: 0.5443 - val_accuracy: 0.7621 - val_loss: 0.4882\n",
      "Epoch 3/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 279ms/step - accuracy: 0.7888 - loss: 0.4555 - val_accuracy: 0.7521 - val_loss: 0.4987\n",
      "Epoch 4/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 299ms/step - accuracy: 0.8072 - loss: 0.4090 - val_accuracy: 0.8150 - val_loss: 0.3997\n",
      "Epoch 5/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 261ms/step - accuracy: 0.8455 - loss: 0.3392 - val_accuracy: 0.8314 - val_loss: 0.3718\n",
      "Epoch 6/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 255ms/step - accuracy: 0.8661 - loss: 0.2957 - val_accuracy: 0.8521 - val_loss: 0.3371\n",
      "Epoch 7/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 278ms/step - accuracy: 0.8973 - loss: 0.2443 - val_accuracy: 0.8607 - val_loss: 0.3198\n",
      "Epoch 8/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 282ms/step - accuracy: 0.9140 - loss: 0.2105 - val_accuracy: 0.8650 - val_loss: 0.3241\n",
      "Epoch 9/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 265ms/step - accuracy: 0.9286 - loss: 0.1690 - val_accuracy: 0.8657 - val_loss: 0.3263\n",
      "Epoch 10/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 267ms/step - accuracy: 0.9423 - loss: 0.1432 - val_accuracy: 0.8743 - val_loss: 0.3203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29e6710ac30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738d8466-571d-4333-9aa7-4ad74280055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.8756 - loss: 0.3225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3203279376029968, 0.8742856979370117]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(xtest,ytest)\n",
    "# model.save('datathonModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f927fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loadedModel = load_model('datathonModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d0ee329-58c2-47c0-a5e6-bbec62178cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The layer sequential_1 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad-CAM Overlay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Generate Grad-CAM visualization for the first test imagek\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[43mgenerate_gradcam_for_test_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mpredict(xtest[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m1\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[12], line 81\u001b[0m, in \u001b[0;36mgenerate_gradcam_for_test_sample\u001b[1;34m(sample_index)\u001b[0m\n\u001b[0;32m     78\u001b[0m test_image_expanded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(test_image, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Get Grad-CAM heatmap\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mmake_gradcam_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_image_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconv2d_3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Display the original image and Grad-CAM overlay\u001b[39;00m\n\u001b[0;32m     84\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m, in \u001b[0;36mmake_gradcam_heatmap\u001b[1;34m(model, img_array, layer_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03mGenerate a Grad-CAM heatmap for the given image and model.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m- heatmap: A 2D heatmap showing important regions for prediction.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a model that maps input to the desired conv layer's output and predictions\u001b[39;00m\n\u001b[0;32m     17\u001b[0m grad_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[1;32m---> 18\u001b[0m     [\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m], \n\u001b[0;32m     19\u001b[0m     [model\u001b[38;5;241m.\u001b[39mget_layer(layer_name)\u001b[38;5;241m.\u001b[39moutput, model\u001b[38;5;241m.\u001b[39moutput]\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get the output feature maps and the model's prediction for the input image\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\operation.py:254\u001b[0m, in \u001b[0;36mOperation.input\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\ops\\operation.py:285\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[1;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    294\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The layer sequential_1 has never been called and thus has no defined input."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def make_gradcam_heatmap(model, img_array, layer_name='conv2d_3'):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap for the given image and model.\n",
    "    Arguments:\n",
    "    - model: Trained CNN model.\n",
    "    - img_array: Preprocessed input image array.\n",
    "    - layer_name: Name of the last convolutional layer for Grad-CAM.\n",
    "    Returns:\n",
    "    - heatmap: A 2D heatmap showing important regions for prediction.\n",
    "    \"\"\"\n",
    "    # Create a model that maps input to the desired conv layer's output and predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.input], \n",
    "        [model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Get the output feature maps and the model's prediction for the input image\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        loss = predictions[:, 0]  # Only for the first output (binary classification)\n",
    "\n",
    "    # Compute gradients of the predicted class with respect to the conv layer output\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "    # Pool the gradients to get the importance of each channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Multiply each channel by the pooled gradients to get weighted activation\n",
    "    conv_outputs = conv_outputs[0]  # Remove batch dimension\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    \n",
    "    # heatmap is already a NumPy array, so just return it\n",
    "    return heatmap # Changed from heatmap.numpy() to heatmap\n",
    "\n",
    "def display_gradcam(img, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    Superimpose the Grad-CAM heatmap on the original image.\n",
    "    Arguments:\n",
    "    - img: Original image.\n",
    "    - heatmap: 2D Grad-CAM heatmap.\n",
    "    - alpha: Opacity factor for heatmap overlay.\n",
    "    \"\"\"\n",
    "    # Resize the heatmap to match the input image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Convert heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert the grayscale image to RGB to match the heatmap's channels\n",
    "    # Convert img to uint8 before applying cvtColor\n",
    "    img = np.uint8(img * 255)  # Convert to 8-bit unsigned integer\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)  # Convert grayscale to RGB\n",
    "\n",
    "    # Superimpose heatmap on the original image\n",
    "    superimposed_img = heatmap * alpha + img\n",
    "\n",
    "    plt.imshow(superimposed_img.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "# Example usage:\n",
    "def generate_gradcam_for_test_sample(sample_index=0):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM visualization for a test sample.\n",
    "    Arguments:\n",
    "    - sample_index: Index of the test image for Grad-CAM visualization.\n",
    "    \"\"\"\n",
    "    # Select a test image and its corresponding label\n",
    "    test_image = xtest[sample_index]\n",
    "    test_image_expanded = np.expand_dims(test_image, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Get Grad-CAM heatmap\n",
    "    heatmap = make_gradcam_heatmap(model, test_image_expanded, layer_name='conv2d_3')\n",
    "\n",
    "    # Display the original image and Grad-CAM overlay\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_image.squeeze(), cmap='gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    display_gradcam(test_image.squeeze(), heatmap)\n",
    "    plt.title(\"Grad-CAM Overlay\")\n",
    "\n",
    "# Generate Grad-CAM visualization for the first test imagek\n",
    "generate_gradcam_for_test_sample(1)\n",
    "print(model.predict(xtest[0].reshape(1,128,128,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cf502",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 128, 128, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxtest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ytest[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ajay\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (1, 128, 128, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 128, 128, 1), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "print(model.predict(xtest[0].reshape(1,128,128,1)))\n",
    "print(ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e31f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay\\AppData\\Local\\Temp\\ipykernel_2840\\3857035851.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  predicted_class = int(confidence_score>=0.5)\n"
     ]
    }
   ],
   "source": [
    "confidence_score  = model.predict(xtest[0].reshape(1,128,128,1))\n",
    "predicted_class = int(confidence_score>=0.5)\n",
    "# print(f\"Confidence = {confidence_score:.4f}\")\n",
    "print(predicted_class)\n",
    "print(ytest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaba9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Prediction: [[0.99872625]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the image and convert it to grayscale\n",
    "img_path = r'D:\\SIH project\\dataset2\\Dataset\\Train\\Fake\\fake_26.jpg'\n",
    "def imgToArray(img_path):\n",
    "    img = image.load_img(img_path, target_size=(128, 128))  # Resize\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Add batch dimension and normalize\n",
    "    img_array = np.expand_dims(img_array, axis=(0, -1))  # Shape: (1, 128, 128, 1)\n",
    "    img_array = img_array / 255.0  # Normalize the pixel values\n",
    "    return img_array\n",
    "img_array = imgToArray(img_path)\n",
    "# Pass the image through the model\n",
    "predictions = model.predict(img_array)\n",
    "print(\"Prediction:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d3e7f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction: [[0.99872625]]\n",
      "[[0.00010955]]\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"D:\\SIH project\\modified_image.jpg\"\n",
    "newArray = imgToArray(img_path)\n",
    "\n",
    "newPredictions = model.predict(newArray)\n",
    "print(\"Prediction:\", predictions)\n",
    "\n",
    "print(predictions-newPredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
